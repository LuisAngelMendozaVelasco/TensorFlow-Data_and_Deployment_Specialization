# Toxicity Classifier

In the next example, we will use the pre-trained Toxicity model to detect whether a given piece of text contains toxic content such as threatening language, insults, obscenities, identity-based hate, or sexually explicit language. 

You can use Brackets to open the **C1_W3_Lab_1_toxicity_classifier.html** file and take a look at the code. You can find the **C1_W3_Lab_1_toxicity_classifier.html** file in the following folder in the GitHub repository for this course:

https://github.com/https-deeplearning-ai/tensorflow-2-public/tree/main/C1_Browser-based-TF-JS/W3/ungraded_labs

When you launch the **C1_W3_Lab_1_toxicity_classifier.html** file in the Chrome browser make sure to open the Developer Tools to see the output in the Console.